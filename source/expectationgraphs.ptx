<section xml:id="sec-expectationgraphs">
    <title>Expected Values: Graphs</title>

	<p>
		We next apply the tools we have developed to investigate expected values of random variables defined on random graphs.
		We begin with the Erdos-Renyi random graph model <m>G(n,p)</m>, which is defined as follows.
	</p>

	<definition xml:id="def-erdosrenyi">
		<statement>
			<p>
				Let <m>n</m> be a positive integer and <m>p</m> be a real number with <m>0 \lt p \lt 1</m>.
				The <em>Erdos-Renyi random graph model</em> <m>G(n,p)</m> is the probability space whose set of events consists of all graphs with vertex set <m>\{1,2,\ldots,n\}</m>, where each edge is included independently with probability <m>p</m>.
				Thus, the probability of a particular graph <m>G</m> with <m>e</m> edges is <m>p^e (1-p)^{\binom{n}{2}-e}</m>.
			</p>
		</statement>
	</definition>

	<p>
		The Erdos-Renyi model is one of the most well-studied random graph models. 
		It is actually a binomial distribution in disguise, since it is equivalent to considering binary strings of length <m>\binom{n}{2}</m>, where each bit corresponds to the presence or absence of a particular edge, as the following example illustrates.
	</p>

	<example>
		<p>
			Let <m>n=4</m> and <m>p=\frac{1}{2}</m>.
			Then the probability space <m>G(4,\frac{1}{2})</m> consists of all graphs with vertex set <m>\{1,2,3,4\}</m>, where each edge is included independently with probability <m>\frac{1}{2}</m>.
			There are <m>\binom{4}{2}=6</m> possible edges, so there are <m>2^6=64</m> possible graphs in this model, each with probability <m>\left(\frac{1}{2}\right)^6=\frac{1}{64}</m>.
			Thus, this model gives the uniform distribution on the set of all graphs with vertex set <m>\{1,2,3,4\}</m>.
			Further, each graph can be represented by a binary string of length <m>6</m>, where we index the entries of the string by the edges <m>12, 13, 14, 23, 24, 34</m>.
			Then, the entry is <m>1</m> if the corresponding edge is present, and <m>0</m> otherwise.
			For example, the graph with edges <m>12</m>, <m>14,</m> and <m>23</m> corresponds to the binary string <m>101100</m>.
		</p>
	</example>

	<p>
		Using this connection to the binomial distribution, we can compute expected values of various graph statistics in the Erdos-Renyi model.
		We illustrate this with the following theorem.
	</p>
	
	<theorem xml:id="thm-erdosrenyiexpectednumberedges">
		<statement>
			<p>
				The expected number of edges in a graph drawn from the Erdos-Renyi model <m>G(n,p)</m> is <m>p \binom{n}{2}</m>.
			</p>
		</statement>
	</theorem>

	<proof>
		<p>
			Using the fact that <m>G(n,p)</m> is equivalent to a binomial distribution on binary strings of length <m>\binom{n}{2}</m> and probability <m>p</m>, we have that the expected number of edges is <m>p \binom{n}{2}</m>.
		</p>
	</proof>

	<p>
		We can also investigate properties of Erdos-Renyi graphs that do not immediately make sense for only binary strings.
		This is because binary strings do not have an automatic relationship between the different positions in the string, but graphs do via their vertices and edges, and the binary strings corresponding to graphs in the Erdos-Renyi model respect this structure.
	</p>

	<theorem xml:id="thm-erdosrenyiisolated">
		<statement>
			<p>
				The expected number of isolated vertices in a graph drawn from the Erdos-Renyi model <m>G(n,p)</m> is <m>n(1-p)^{n-1}</m>.
			</p>
		</statement>
	</theorem>

	<proof>
		<p>
			Let <m>X</m> be the random variable giving the number of isolated vertices in a graph drawn from <m>G(n,p)</m>.
			For <m>1 \leq i \leq n</m>, let <m>X_i</m> be the indicator random variable for the event that vertex <m>i</m> is isolated.
			Then we have
			<me>
				X = X_1 + X_2 + \cdots + X_n.
			</me>
			By linearity of expectation, we have
			<me>
				E[X] = E[X_1] + E[X_2] + \cdots + E[X_n].
			</me>
			Now, for each <m>i</m>, we have
			<me>
				E[X_i] = P(X_i = 1) = (1-p)^{n-1},
			</me>
			since for vertex <m>i</m> to be isolated, none of the <m>n-1</m> edges connecting it to the other vertices can be present, and each edge is absent with probability <m>1-p</m>.
			Thus, we have
			<me>
				E[X] = n(1-p)^{n-1}.
			</me>
		</p>
	</proof>

	<exercise>
		<p>
			Discuss the proof above. Does it make sense? Why or why not?
		</p>
	</exercise>

	<p>
		We can also compute the expected number of triangles, i.e., the number of <m>K_3</m>'s, in an Erdos-Renyi graph.
	</p>

	<theorem xml:id="thm-erdosrenyitriangles">
		<statement>
			<p>
				Let <m>X</m> be the random variable giving the number of triangles in a graph drawn from the Erdos-Renyi model <m>G(n,p)</m>.
				Then <me>E[X]=\binom{n}{3} p^3 \, .</me>
			</p>
		</statement>
	</theorem>

	<proof>
		<p>
			Let <m>X</m> be the random variable giving the number of triangles in a graph drawn from <m>G(n,p)</m>.
			For <m>1 \leq i \lt j \lt k \leq n</m>, let <m>X_{i,j,k}</m> be the indicator random variable for the event that the vertices <m>i, j, k</m> form a triangle, i.e., that the edges <m>\{i,j\},\{i,k\},\{j,k\}</m> are all present in the graph.
			Then we have
			<me>
				X = \sum_{1 \leq i \lt j \lt k \leq n} X_{i,j,k}.
			</me>
			By linearity of expectation, we have
			<me>
				E[X] = \sum_{1 \leq i \lt j \lt k \leq n} E[X_{i,j,k}].
			</me>
			Now, for each triple <m>(i,j,k)</m>, we have
			<me>
				E[X_{i,j,k}] = P(X_{i,j,k} = 1) = p^3,
			</me>
			since for vertices <m>i, j, k</m> to form a triangle, all three edges connecting them must be present, and each edge is present with probability <m>p</m>.
			Thus, we have
			<me>
				E[X] = \binom{n}{3} p^3.
			</me>
		</p>
	</proof>

	<exercise>
		<p>
			Discuss the proof above. Does it make sense? Why or why not?
		</p>
	</exercise>
	
	<p>
		It is good to get in the habit of checking whether your expected value results make sense.
		For example, in the last theorem, as <m>p</m> approaches <m>0</m>, the expected number of triangles approaches <m>0</m>, which makes sense since there are fewer edges in the graph.
		Similarly, as <m>p</m> approaches <m>1</m>, the expected number of triangles approaches <m>\binom{n}{3}</m>, which makes sense since the graph is approaching the complete graph <m>K_n</m>, which has <m>\binom{n}{3}</m> triangles.
	</p>

	<p>
		We can also get in the habit of comparing our expected value results to actual values obtained from simulations.
		For example, we can simulate drawing graphs from <m>G(10, p)</m> and counting the number of triangles in each graph.
		Doing this for various values of <m>p</m> should give values close to those predicted by our theorem.
		Note that <me>\binom{10}{3} = 120</me>, so the expected number of triangles in <m>G(10,p)</m> is <m>120 p^3</m>.
		How does this compare to simulations where we randomly generate <m>1000</m> graphs from <m>G(10,p)</m> for <m>p=0.1,0.2,\ldots , 0.9</m> and compute the average number of triangles per graph for each sample?
	</p>

	<figure xml:id="fig-trianglesexperiment">
    <caption>Plot of experimental expected value of number of triangles versus <m>p</m> overlaid with the plot of <m>y=\binom{10}{3}p^3</m>.</caption>
    <image source="trianglesexperiment.png" width="100%">
        <shortdescription>Plot of experimental expected value of number of triangles versus p.</shortdescription>
    </image>
</figure>

<exercise>
	<p>
		Discuss the simulation results above. Do they make sense? Why or why not?
	</p>
</exercise>

<p>
	Let's look at two more examples of computing expected values in the Erdos-Renyi model.
	First, we will consider the expected average degree of a vertex.
</p>

<theorem xml:id="thm-expectedaveragedegree">
	<statement>
		<p>
			Let <m>X</m> be the random variable giving the average degree of a vertex in a graph drawn from the Erdos-Renyi model <m>G(n,p)</m>.
			Then <me>E[X]=p(n-1)</me>.
		</p>
	</statement>
</theorem>

<proof>
	<p>
		Let <m>X</m> be the random variable giving the average degree of a vertex in a graph drawn from <m>G(n,p)</m>.
		By definition, we have
		<me>
			X = \frac{\deg(1) + \deg(2) + \cdots + \deg(n)}{n},
		</me>
		where <m>\deg(i)</m> is the degree of vertex <m>i</m>.
		By linearity of expectation, we have
		<me>
			E[X] = \frac{E[\deg(1)] + E[\deg(2)] + \cdots + E[\deg(n)]}{n}.
		</me>
		Now, for each <m>i</m>, we have
		<me>
			E[\deg(i)] = p(n-1),
		</me>
		since the degree of vertex <m>i</m> is the sum of <m>n-1</m> indicator random variables for the presence of each edge connecting vertex <m>i</m> to the other vertices, each of which has expected value <m>p</m>.
		Thus, we have
		<me>
			E[X] = p(n-1).
		</me>
	</p>
</proof>

<exercise>
	<p>
		Discuss the proof above. Does it make sense? Why or why not?
	</p>
</exercise>

<p>
	As our final example, we will find the expected number of spanning trees in an Erdos-Renyi graph.
</p>

<theorem xml:id="thm-expectedspanningtrees">
	<statement>
		<p>
			Let <m>T</m> be the random variable giving the number of spanning trees in a graph drawn from the Erdos-Renyi model <m>G(n,p)</m>.
			Then <me>E[T]=p^{n-1} n^{n-2}</me>.
		</p>
	</statement>
</theorem>

<proof>
	<p>
		Let <m>T</m> be the random variable giving the number of spanning trees in a graph drawn from <m>G(n,p)</m>.
		For each spanning tree <m>S</m> on vertex set <m>\{1,2,\ldots,n\}</m>, let <m>T_S</m> be the indicator random variable for the event that spanning tree <m>S</m> is present in the graph.
		Then we have
		<me>
			T = \sum_{S} T_S,
		</me>
		where the sum is over all spanning trees <m>S</m> on vertex set <m>\{1,2,\ldots,n\}</m>.
		By linearity of expectation, we have
		<me>
			E[T] = \sum_{S} E[T_S].
		</me>
		Now, for each spanning tree <m>S</m>, we have
		<me>
			E[T_S] = P(T_S = 1) = p^{n-1},
		</me>
		since a spanning tree on <m>n</m> vertices has <m>n-1</m> edges, and each edge is present with probability <m>p</m>.
		By Cayley's formula, there are <m>n^{n-2}</m> spanning trees on vertex set <m>\{1,2,\ldots,n\}</m>.
		Thus, we have
		<me>
			E[T] = p^{n-1} n^{n-2}.
		</me>
	</p>
</proof>

<exercise>
	<p>
		Discuss the proof above. Does it make sense? Why or why not?
	</p>
</exercise>

<p>
	Let's do one final check to see if this result makes sense.
	As <m>p</m> approaches <m>0</m>, the expected number of spanning trees approaches <m>0</m>, which makes sense since there are fewer edges in the graph.
	Similarly, as <m>p</m> approaches <m>1</m>, the expected number of spanning trees approaches <m>n^{n-2}</m>, which makes sense since the graph is approaching the complete graph <m>K_n</m>, which has <m>n^{n-2}</m> spanning trees by Cayley's formula.
	Finally, we can simulate drawing graphs from <m>G(10, p)</m> and counting the number of spanning trees in each graph, as we did for triangles above.
</p>

<figure xml:id="fig-spanningtreesexperiment">
    <caption>Plot of experimental expected value of number of spanning trees versus <m>p</m> overlaid with the plot of <m>y=10^8p^9</m>.</caption>
    <image source="spanningtreesexperiment.png" width="100%">
        <shortdescription>Plot of experimental expected value of number of spanning trees versus p overlaid with the plot of y=10^8p^9.</shortdescription>
    </image>
</figure>

	
    
</section>